---
format: 
  revealjs: 
    chalkboard: true
    logo: logos_ine.png
    incremental: true
    slide-number: true
    scrollable: false
    css: custom.css
---

```{r}
#| echo: false
# Configuración global
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center")

# Paquetes necesarios
pacman::p_load(
  flextable,
  patchwork,
  performance,
  emmeans,
  rio,
  janitor,
  tidyverse
)
```

## Módulo VI: Análisis Epidemiológico Avanzado {.center style="text-align: center;"}

:::: {style="margin-top: 1.5em;"}
Docentes: Tamara Ricardo, Christian Ballejo

::: {style="font-size: .75em; margin-bottom: 1.5em;"}
Programa de Maestría en Epidemiología para la Salud Pública
:::

![](logos_ine.png){fig-align="center" width="70%"}
::::

## PROGRAMA DE LA UNIDAD

::: {style="margin-top:1.5em;"}
```{r}
# Genera datos para la tabla 
tibble(
  Semana = c(rep("30 oct. al 03 nov. 2024", 2), 
             rep("06 al 09 nov. 2024", 2),
             "13 al 16 nov. 2024", ""),      
  
      Tema = c("- Introducción a los Paquetes y Lenguajes Estadístico. Diferencias entre interfaces gráficas (GUI) y de línea de comandos (CLI). Comparativa entre software privativo y gratuito/open source.",
               "- R y R-Commander. Navegación del menú. Lectura e importación de archivos de datos. Estadística descriptiva. Agrupamiento de variables. Manejo de factores. Guardado de scripts y resultados. Paquetes y plugins.",
               "- Relación entre variables numéricas. Covarianza y representación gráfica. Limitaciones. Correlación de Pearson: interpretación del signo y la magnitud.Visualización con correlogramas. Métodos no paramétricos: correlación de Spearman y de Kendall.",
               "- Introducción al Modelado Estadístico. Modelo lineal general: concepto y supuestos. Bondad de ajuste y análisis de residuos. Regresión lineal simple y análisis de la varianza (ANOVA). Interpretación de los resultados.",
               "- Regresión Lineal Múltiple. Selección de variables explicativas y control de multicolinealidad. Análisis e interpretación de residuos.",
               "- Confusión e Interacción. Identificación y roles de las covariables. Control y detección de la confusión. Interpretación de resultados en presencia de interacción.")
      ) |>    
  
  #  Genera tabla    
  flextable() |>    
  bold(part = "header") |> 
  fontsize(size = 16, part = "all") |>   
  merge_v() |>  
  color(i = c(1:3, 5:6), color = "gray40") |> 
  highlight(i = 4, color = "#FFB90F") |> 
  autofit() 
```
:::

## ESTRUCTURA DE LA CLASE

::: {style="margin-top: 2em;"}
```{r}
# Genera datos para la tabla
tibble(
  Tiempo = c("18:30hs", 
             "18:40hs", 
             "20:00hs",
             "20:15hs", 
             "21:30hs"),
  
  Descripción = c("Ingreso a la videollamada", 
                  "Inicio de la clase",
                  "Receso", 
                  "Continuación clase", 
                  "Cierre")
) |> 
  
  #  Genera tabla    
  flextable() |>    
  bold(part = "header") |> 
  fontsize(size = 20, part = "all") |>   
  merge_v() |>  
  autofit() 
```
:::

## OBJETIVOS

-   Comprender los **fundamentos** del modelo lineal general y su aplicación.
-   **Ajustar modelos lineales** utilizando R Commander.
-   Interpretar los **coeficientes** de un modelo lineal.
-   Evaluar la **bondad de ajuste** con el coeficiente de determinación $R^2$.
-   **Verificar supuestos** del modelo de relación lineal.

## INTRODUCCIÓN

-   La **estadística inferencial** se basa en modelos para sacar conclusiones o realizar predicciones sobre el comportamiento de fenómenos poblacionales a partir de datos muestrales.

-   Los **modelos estadísticos** son representaciones matemáticas que intentan **describir, explicar** y **predecir** de la manera más sencilla posible la relación entre dos o más variables.

------------------------------------------------------------------------

### Modelos **determinísticos**

-   Son modelos estadísticos que asumen relaciones **exactas** entre variables.

-   Los parámetros y variables se conocen **sin error**.

-   Siempre producen el **mismo resultado para un mismo conjunto de datos.**

-   **No incorporan** la variabilidad causada por el azar o la incertidumbre.

------------------------------------------------------------------------

**Modelo SIR** (Susceptibles, Infectados, Recuperados)

-   Se utiliza para modelar la **propagación de enfermedades** en poblaciones cerradas.

-   Asume tasas de transmisión y recuperación **constantes**.

-   Limitaciones:

    -   No considera la **variabilidad individual**.

    -   No pueden aplicarse a **poblaciones abiertas** (con migraciones o nacimientos).

    -   Resultados sensibles a cambios en **parámetros estimados**.

------------------------------------------------------------------------

### Modelos **probabilísticos** (***regresión*****)**

-   Asumen que los fenómenos observados no son totalmente predecibles, y que ciertas variables están sujetas al azar (**componente aleatorio**).

-   No predicen un único resultado; en cambio, generan una **distribución de posibles resultados**.

-   Los resultados para un conjunto de datos pueden variar cada vez que se ejecute el modelo.

-   Los resultados se expresan en términos de probabilidades.

------------------------------------------------------------------------

### Componentes de un modelo de regresión

-   **Variable dependiente** ($Y$): también llamada variable respuesta o resultado, representa el **efecto observado** de un proceso o interacción causal.

-   **Variable/s independiente/s** ($X$) : también llamadas variables explicativas o predictores, representan la **influencia del proceso o interacción** que se está midiendo sobre la variable dependiente.

-   **Error aleatorio** ($\epsilon$): **variabilidad no explicada** por las variables independientes, atribuible a diferencias individuales y/o a errores de medición o calibración. Puede controlarse aumentando el tamaño de muestra.

------------------------------------------------------------------------

#### **Modelos probabilísticos paramétricos**

-   Asumen que los datos provienen de una distribución con un número fijo de parámetros (media, varianza, etc.).
-   Requieren hacer suposiciones explícitas sobre la forma de la distribución de los datos.
-   Ejemplos:
    -   Regresión lineal
    -   Análisis de la varianza (ANOVA)
    -   Regresión logística

------------------------------------------------------------------------

#### **Modelos probabilísticos no paramétricos**

-   Se utilizan para analizar datos cuya distribución subyacente es desconocida o no sigue una distribución estándar.

-   Son más flexibles pero requieren de mayor cantidad de datos.

-   Ejemplos:

    -   Curvas de Kaplan Meier

    -   Regresión de Cox (semiparamétrica)

------------------------------------------------------------------------

## Modelo lineal general

-   Modela la relación lineal entre una variable respuesta numérica con distribución normal ($Y$) y una o más variables independientes.

-   Su marco estadístico incluye:

    -   **Regresión lineal simple**: relación entre $Y$ y una variable independiente numérica.

    -   **Análisis de la varianza (ANOVA)**: diferencia en la media de $Y$ respecto a los niveles de una variable categórica.

    -   **Regresión múltiple** y **análisis de la covarianza**: relación entre $Y$ y 2 o más variables independientes.

------------------------------------------------------------------------

-   Matemáticamente se representa como:

    $$
    Y_i = X_i\beta + \epsilon_i
    $$

-   Donde:

    -   $Y_i$ : vector de valores de la variable dependiente.

    -   $X_i$: matriz de diseño que contiene las variables independientes.

    -   $\beta$: vector de coeficientes (parámetros).

    -   $\epsilon_i$: error aleatorio.

------------------------------------------------------------------------

### Supuestos del modelo lineal general

-   **Linealidad**: La relación entre la variable dependiente y cada variable independiente es lineal.

-   **Independencia**: Las observaciones deben ser independientes entre sí.

-   **Homoscedasticidad**: La varianza de los errores es constante para todos los valores de las variables independientes.

-   **Normalidad de los errores**: Los errores (residuales) del modelo deben seguir una distribución normal.

------------------------------------------------------------------------

### Regresión lineal simple

-   Expresa la relación lineal entre $Y$ y una variable independiente continua ($X$).
-   Veamos el *scatterplot* para la covarianza entre la tasa de mortalidad por cáncer (`tasa_mortalidad`) y desempleo (`pct_desempleo`).

------------------------------------------------------------------------

#### *Scatterplot*: tasa de mortalidad vs. desempleo

```{r}
# Carga datos
datos <- read.csv2("../cancer_USA.txt")

# Genera scatter plot
datos |> 
  ggplot(mapping = aes(x = pct_desempleo, 
                       y = tasa_mortalidad)) +
  
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  
  labs(x = "Desempleo (%)", 
       y = "Tasa de mortalidad por cáncer") +
  theme_minimal()
```

------------------------------------------------------------------------

-   La relación entre estas dos variables podría expresarse mediante la ecuación de la recta:

    $$ Y = \beta_0 + \beta_1X + \epsilon $$

-   Donde:

    -   $Y$ es la variable dependiente (`tasa_mortalidad`).

    -   $X$ es la variable independiente (`pct_desempleo`).

    -   $\beta_0$ es el intercepto.

    -   $\beta_1$ es la pendiente de la recta.

    -   $\epsilon$ es el término error aleatorio.

------------------------------------------------------------------------

#### Definamos ahora los términos de la ecuación...

-   **Intercepto** ($\beta_0$):

    -   Representa el valor esperado de $Y$ cuando $X=0$.
    -   Generalmente no tiene una interpretación realista.

-   **Pendiente** ($\beta_1$):

    -   Mide la tasa de cambio $Y$ por unidad de cambio de $X$.
    -   Si $\beta_1$ toma valores positivos, $X$ e $Y$ tienen **relación directa**, mientras que si $\beta_1$ es negativa se trata de una **relación inversa**.
    -   Sobre este término se basan las hipótesis del modelo:
        -   $H_0: \beta_1 = 0$ (No hay relación entre $X$ e $Y$)
        -   $H_1: \beta_1 \not= 0$ (Relación lineal entre $X$ e $Y$)

------------------------------------------------------------------------

-   **Error aleatorio** ($\epsilon$)

    -   Variación en la medición de un valor que se debe al azar. 

    -   El modelo lineal asume que los errores son independientes y están idénticamente distribuidos (i.i.d) y siguen una distribución normal:

        $$
        \epsilon \stackrel{i.i.d}{\sim} N(0, \sigma^2)
        $$

------------------------------------------------------------------------

### Método de los mínimos cuadrados

-   Técnica estadística para encontrar la **línea de regresión** que mejor ajusta un conjunto de datos.

-   Se basa en minimizar la suma de los cuadrados de las diferencias (**errores**) entre los valores observados y los valores predichos por el modelo.

-   La precisión del ajuste se evalúa usando el coeficiente de determinación ($R^2$).

------------------------------------------------------------------------

-   Puede visualizarse en el *scatterplot* agregando la distancia los valores observados y los predichos:

    ```{r}
    # Modifica datos
    datos1 <- sample_n(datos, 25)

    # Ajusta modelo
    fit <- lm(tasa_mortalidad ~ pct_desempleo, data = datos1)

    # Añade datos ajustados
    datos1 <- datos1 |> 
      mutate(y_fit = predict(fit))

    # Genera scatter plot con residuales
    datos1 |> 
      ggplot(mapping = aes(x = pct_desempleo, 
                           y = tasa_mortalidad)) +
      
      geom_segment(aes(xend = pct_desempleo, yend = y_fit),
                   linetype = "dashed") +
      
      geom_point(color = "#105185", size = 3, alpha = .8) +
      geom_smooth(method = "lm", se = F, color = "#021326") +
      
      
      labs(x = "Desempleo (%)", 
           y = "Tasa de mortalidad por cáncer") +
      theme_minimal()
    ```

------------------------------------------------------------------------

#### Coeficiente de determinación ($R^2$)

-   Indica la proporción de la variabilidad de $Y$ que explica el modelo de regresión lineal:

    $$
    R^2 = 1- \frac{STC}{SCE}
    $$

    -   Donde:

        -   **SCE** es la suma de errores cuadrados, es decir, la diferencia entre los valores observados y los predichos por el modelo.
        -   **STC** es la suma total de cuadrados, variabilidad total de los valores observados con respecto a su media.

------------------------------------------------------------------------

**Interpretación**

-   $R^2 = 1$: indica un ajuste perfecto, donde el modelo explica el 100% de la variabilidad de $Y$.

-   $R^2 = 0$: significa que el modelo no explica ninguna variabilidad de $Y$, y es equivalente a tomar la media de $Y$ como predicción constante.

-   **Valores intermedios**: cuanto más cerca esté $R^2$ de 1, mejor será el ajuste del modelo.

::: {.callout-important .fragment appearance="simple"}
## Aunque un valor alto de $R^2$ indica un buen ajuste, no garantiza que el modelo sea adecuado, ya que podría estar sobreajustado o faltar una relación causal.
:::

------------------------------------------------------------------------

### Residuales

-   Los residuos o residuales representan la diferencia entre los valores observados y los ajustados por el modelo.
-   Tienen las siguientes características:
    -   **Promedio cero**: La suma de los residuales de un modelo de regresión lineal debe ser igual a cero.
    -   **Normalidad**: Se espera que los residuales se distribuyan normalmente.
    -   **Independencia**: No debe haber patrones evidentes en su distribución.
    -   **Homocedasticidad**: No debe haber patrones sistemáticos en la dispersión de los residuales.

------------------------------------------------------------------------

-   Los residuales se analizan gráficamente usando las siguientes herramientas:

    -   **Gráficos Q-Q**: Comparan la distribución de los residuales con una distribución normal teórica.

    -   **Histogramas:** Ayudan a visualizar si los residuales se distribuyen normalmente.

    -   **Gráficos de dispersión**: Muestran los residuales frente a los valores predichos. Un patrón aleatorio sugiere independencia y homocedasticidad.

    -   **Gráficos de *outliers***: muestran la presencia de valores extremos (*outliers*) o influyentes que puedan distorsionar las estimaciones del modelo.

------------------------------------------------------------------------

#### Normalidad y linealidad

```{r}
# Modelo lineal
fit <- lm(tasa_mortalidad ~ pct_desempleo, data = datos)

# Gráfico normalidad y linealidad
check_model(fit, check = c("qq", "linearity"))
```

------------------------------------------------------------------------

#### Homocedasticidad y valores extremos

```{r}
# Gráfico homocedasticidad y outliers
check_model(fit, check = c("homogeneity", "outliers"))
```

## Ejemplo en R Commander

-   Activar RCommander (`library(Rcmdr`) y el plugin `KMggplot2`.

    ![](images/library_rcmdr.png){width="70%"}

------------------------------------------------------------------------

-   Cargar la base de datos "`cancer_USA.txt`"

    ![](images/import.png)

------------------------------------------------------------------------

-   Vamos a **Estadísticos \> Ajuste de Modelos \> Modelo Lineal**.

    ![](images/mod_lineal.png)

------------------------------------------------------------------------

-   En la nueva ventana seleccionamos `tasa_mortalidad` como variable dependiente y `mediana_edad` como variable explicativa y presionamos **Aceptar** o **Aplicar**:

    ![](images/mod_lineal2.png)

------------------------------------------------------------------------

-   **Obtendremos la siguiente salida**

    ```{r}
    mod1 <- lm(tasa_mortalidad ~ mediana_edad, data = datos)

    summary(mod1)
    ```

------------------------------------------------------------------------

-   `Call`: es la fórmula que usamos para definir el modelo.

-   `Residuals`: resumen de la distribución de los residuales (mínimo, Q1, mediana, Q3, máximo).

-   `Coefficients`:

    -   `Estimate`: valores del intercepto y de la pendiente

    -   `Std. error`: errores estándar del intercepto y la pendiente.

    -   `t value`: estadístico $t$

    -   `Pr (>|t|)`: *p*-valor de probabilidad dada la hipótesis nula que los coeficientes sean iguales a cero.

------------------------------------------------------------------------

-   `Residual standard error`: Error estándar de los residuos con sus grados de libertad

-   `Multiple R-squared`: Coeficiente de determinación $R^2$

-   `Adjusted R-squared`: Coeficiente $R^2$ ajustado

-   `F-statistic`: estadístico $F$ sobre la hipótesis nula que el cociente entre la varianza de la ecuación de regresión y la varianza de los residuos es igual a 1.

-   `p-value`: p-valor del estadistico $F$.

------------------------------------------------------------------------

-   Si en lugar de los coeficientes y sus errores estándar quisiéramos visualizar los intervalos de confianza, debemos calcularlos desde **Modelos \> Intervalos de Confianza:**

    ![](images/confint.png)

------------------------------------------------------------------------

-   Se abrirá la siguiente ventana:

    ![](images/confint2.png)

-   Presionamos **Aceptar** y aparecerá la siguiente salida:

    ```{r}
    confint(mod1)
    ```

------------------------------------------------------------------------

-   Podemos chequear si la relación lineal es estadísticamente significativa desde **Modelos \> Test de hipótesis \> Tabla ANOVA**

    ![](images/anova.png){width="1365"}

------------------------------------------------------------------------

-   Nos aparecerá una nueva ventana:

    ![](images/anova2.png)

-   Presionamos **Aceptar** y obtendremos la siguiente salida:

    ```{r}
    anova(mod1)
    ```

------------------------------------------------------------------------

-   Para el análisis de residuales vamos a **Herramientas \> Cargar paquetes** y seleccionamos la librería `performance`.

    ![](images/performance.png)

------------------------------------------------------------------------

-   A continuación escribimos el siguiente comando y presionamos **Ejecutar**:

    `check_model(mod1)`

-   Se nos abrirá una nueva ventana con la salida del análisis de residuales.

------------------------------------------------------------------------

```{r}
check_model(mod1)
```

------------------------------------------------------------------------

-   Podemos obtener los resultados del test de normalidad escribiendo el siguiente código y presionando **Ejecutar:**

    `check_normality(mod1)`

-   Nos aparecerá la siguiente salida:

    ```{r}
    check_normality(mod1)
    ```

------------------------------------------------------------------------

-   Para el test de homogeneidad de varianzas escribimos el siguiente código y presionamos **Ejecutar:**

    `check_heteroscedasticity(mod1)`

-   Nos aparecerá la siguiente salida:

    ```{r}
    check_heteroscedasticity(mod1)
    ```

------------------------------------------------------------------------

-   Para evaluar la presencia de valores extremos, escribimos el siguiente código y presionamos **Ejecutar:**

    `check_outliers(mod1)`

-   Nos aparecerá la siguiente salida:

    ```{r}
    check_outliers(mod1)
    ```

------------------------------------------------------------------------

#### Interpretación

-   La tasa de mortalidad por cáncer (`tasa_mortalidad`) aumenta significativamente (p = 0,0007 ) a medida que aumenta la edad (`mediana_edad`).

-   Este modelo solo explica un porcentaje muy bajo de la variabilidad observada en los datos ($R^2 =$ `r r2(mod1)[2]`).

-   Los residuos cumplen con los supuestos de normalidad, linealidad y homogeneidad de varianza.

-   No se detectaron valores extremos.

------------------------------------------------------------------------

### Análisis de la varianza (ANOVA)

-   Compara la media de la variable independiente $Y$ entre niveles de una variable explicativa categórica ($X$).

-   Al igual que en la regresión lineal simple, se asume que los residuales son normales y homocedásticos.

-   Se basa en la hipótesis estadística de que los grupos son distintos entre sí:

    -   $H_0: \mu_1 = \mu_2 = ... = \mu_i$ (la media de $Y$ no difiere entre grupos)

    -   $H_1: \mu_1 \not= \mu_2 \not= ...\not=\mu_i$ (la media de $Y$ difiere entre grupos)

-   La variabilidad entonces se descompone en variabilidad **entre grupos (SSB)** y variabilidad **dentro de los grupos (SSE)**.

------------------------------------------------------------------------

-   Para evaluar si existen diferencias en las medias de los grupos se utiliza el estadístico $F$ :

    $$
    F = \frac{SSB/(k-1)}{SSE/(n-k)}
    $$

-   donde:

    -   $k$ es el número de grupos

    -   $n$ es el número de observaciones

-   Valores altos de $F$ indican que existen diferencias en las medias de los grupos.

-   Para saber cuáles grupos son diferentes entre sí, hay que realizar tests *post-hoc* de comparaciones múltiples.

------------------------------------------------------------------------

## Ejemplo en R Commander

-   Vamos a **Estadísticos \> Ajuste de Modelos \> Modelo Lineal**.

    ![](images/mod_lineal.png)

<!-- -->

-   En la nueva ventana seleccionamos `tasa_mortalidad` como variable dependiente y `estado` como variable explicativa y apretamos **Aceptar** o **Aplicar**.

-   Nos aparecerá la salida del modelo (ignorar por el momento).

------------------------------------------------------------------------

-   Vamos al menú **Modelos \> Test de hipótesis \> Tabla ANOVA**.

    ![](images/anova.png)

------------------------------------------------------------------------

-   En la nueva ventana presionamos **Aceptar.**

    ![](images/anova2.png)

-   Obtendremos la siguiente salida:

    ```{r}
    mod2 <- lm(tasa_mortalidad ~ estado, data = datos)

    anova(mod2)
    ```

------------------------------------------------------------------------

-   Para realizar el test de comparaciones múltiples cargamos el paquete `emmeans`:

    ![](images/emmeans.png)

------------------------------------------------------------------------

-   En la consola de R Commander creamos el objeto `comp` usando el siguiente código:

    `comp<- emmeans(mod2, specs = "estado", contr = "pairwise")`

-   Presionamos **Ejecutar.**

------------------------------------------------------------------------

-   Para visualizar los resultados del test de comparaciones múltiples escribimos el código:

    `plot(comp)` y presionamos **Ejecutar.**

------------------------------------------------------------------------

-   Nos aparecerá una nueva ventana con el siguiente gráfico:

    ```{r}
    comp<- emmeans(mod2, specs = "estado", contr = "pairwise")

    plot(comp)
    ```

-   Debemos observar entre cuáles grupos **no se superponen** los intervalos de confianza.

------------------------------------------------------------------------

-   Otra opción es escribir `comp` en la consola y **Ejecutar** para que nos aparezca la tabla de comparaciones múltiples.

    ```{r}
    comp$contrasts
    ```

------------------------------------------------------------------------

-   Finalmente chequeamos los supuestos de residuales con `check_model(mod2)`:

    ```{r}
    check_model(mod2)
    ```

------------------------------------------------------------------------

#### Interpretación

-   La tasa de mortalidad por cáncer (`tasa_mortalidad`) difiere significativamente entre los estados de la Costa Este (p \< 0,001 ).

-   Se encontró que la tasa de mortalidad en Connecticut es significativamente menor que en Pennsylvania y Maine.

-   La tasa de mortalidad en Maine es significativamente mayor que en Massachusetts, New Jersey y New York.

-   Los residuos cumplen con los supuestos de normalidad, linealidad y homogeneidad de varianza.

-   No se detectaron valores extremos.

------------------------------------------------------------------------

## Resumiendo...

```{dot}
//| echo: false
digraph G {
 node [style = filled, fontcolor = "white"];
    n0 [shape = ellipse, 
    label = "Variable respuesta \n continua", 
    color = "#1B0D33"];
    
    n1 [shape = box, 
    label = "Intensidad de \n asociación", 
    color = "#4B2044"];
    
    n2 [shape = box, 
    label = "Predicción o \n estimación", 
    color = "#8DC16E"];
    
    n3 [shape = box, 
    label = "Correlación",
    color = "#683F4D"];
    
    n4 [shape = box, 
    label = "Correlación de \n Pearson", 
    color = "#705751"];
    
    n5 [shape = box, 
    label = "Correlación de \n Spearman y \n de Kendall",
    color = "#705751"];
    
   
    n6 [shape = box, 
    label = "Modelo lineal general",
    color = "#7B9859"];
    
    n7 [shape = box,
    label = "ANOVA",
    color = "#757A53"];
    
    n8 [shape = box, 
    label = "Regresión lineal",
    color = "#757A53"];
    
    n0 -> n1;
    n0 -> n2;
    
    n1-> n3 
    n3 -> n4 [labeldistance=2.5, labelangle=90, fontsize=10, headlabel="Paramétrico"];
    n3-> n5 [labeldistance=4.5,labelangle=-70, fontsize=10,headlabel="No \n paramétrico"];
    
    n2 -> n6;
    n6 -> n7;
    n6 -> n8;
}
```
