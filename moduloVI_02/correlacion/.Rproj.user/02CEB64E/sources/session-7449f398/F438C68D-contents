---
format: 
  revealjs: 
    chalkboard: true
    logo: logos_ine.png
    incremental: true
    slide-number: true
    scrollable: false
    css: custom.css
---

```{r}
#| echo: false
# Configuración global
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center")

# Paquetes necesarios
pacman::p_load(
  flextable,
  patchwork,
  performance,
  rio,
  janitor,
  tidyverse
)
```

## Módulo VI: Análisis Epidemiológico Avanzado {.center style="text-align: center;"}

:::: {style="margin-top: 1.5em;"}
Docentes: Tamara Ricardo, Christian Ballejo

::: {style="font-size: .75em; margin-bottom: 1.5em;"}
Programa de Maestría en Epidemiología para la Salud Pública
:::

![](logos_ine.png){fig-align="center" width="70%"}
::::

## PROGRAMA DE LA UNIDAD

::: {style="margin-top:1.5em;"}
```{r}
# Genera datos para la tabla 
tibble(
  Semana = c(rep("30 oct. al 03 nov. 2024", 2), 
             rep("06 al 09 nov. 2024", 2),
             "13 al 16 nov. 2024", ""),      
  
      Tema = c("- Introducción a los Paquetes y Lenguajes Estadístico. Diferencias entre interfaces gráficas (GUI) y de línea de comandos (CLI). Comparativa entre software privativo y gratuito/open source.",
               "- R y R-Commander. Navegación del menú. Lectura e importación de archivos de datos. Estadística descriptiva. Agrupamiento de variables. Manejo de factores. Guardado de scripts y resultados. Paquetes y plugins.",
               "- Relación entre variables numéricas. Covarianza y representación gráfica. Limitaciones. Correlación de Pearson: interpretación del signo y la magnitud.Visualización con correlogramas. Métodos no paramétricos: correlación de Spearman y de Kendall.",
               "- Introducción al Modelado Estadístico. Modelo lineal general: concepto y supuestos. Bondad de ajuste y análisis de residuos. Regresión lineal simple y análisis de la varianza (ANOVA). Interpretación de los resultados.",
               "- Regresión Lineal Múltiple. Selección de variables explicativas y control de multicolinealidad. Análisis e interpretación de residuos.",
               "- Confusión e Interacción. Identificación y roles de las covariables. Control y detección de la confusión. Interpretación de resultados en presencia de interacción.")
      ) |>    
  
  #  Genera tabla    
  flextable() |>    
  bold(part = "header") |> 
  fontsize(size = 16, part = "all") |>   
  merge_v() |>  
  color(i = c(1:3, 5:6), color = "gray40") |> 
  highlight(i = 4, color = "#FFB90F") |> 
  autofit() 
```
:::

## ESTRUCTURA DE LA CLASE

::: {style="margin-top: 2em;"}
```{r}
# Genera datos para la tabla
tibble(
  Tiempo = c("18:30hs", 
             "18:40hs", 
             "20:00hs",
             "20:15hs", 
             "21:30hs"),
  
  Descripción = c("Ingreso a la videollamada", 
                  "Inicio de la clase",
                  "Receso", 
                  "Continuación clase", 
                  "Cierre")
) |> 
  
  #  Genera tabla    
  flextable() |>    
  bold(part = "header") |> 
  fontsize(size = 20, part = "all") |>   
  merge_v() |>  
  autofit() 
```
:::

## OBJETIVOS

-   Comprender los **fundamentos** del modelo lineal general y su aplicación.
-   **Ajustar modelos lineales** utilizando R Commander.
-   Interpretar los **coeficientes** de un modelo lineal.
-   Evaluar la **bondad de ajuste** con el coeficiente de determinación $R^2$.
-   **Verificar supuestos** del modelo de relación lineal.

## INTRODUCCIÓN

-   La **estadística inferencial** se basa en modelos para sacar conclusiones o realizar predicciones sobre el comportamiento de fenómenos poblacionales a partir de datos muestrales.

-   Los **modelos estadísticos** son representaciones matemáticas que intentan **describir, explicar** y **predecir** de la manera más sencilla posible la relación entre dos o más variables.

------------------------------------------------------------------------

### Modelos **determinísticos**

-   Son modelos estadísticos que asumen relaciones **exactas** entre variables.

-   Los parámetros y variables se conocen **sin error**.

-   Siempre producen el **mismo resultado para un mismo conjunto de datos.**

-   **No incorporan** la variabilidad causada por el azar o la incertidumbre.

------------------------------------------------------------------------

**Modelo SIR** (Susceptibles, Infectados, Recuperados)

-   Se utiliza para modelar la **propagación de enfermedades** en poblaciones cerradas.

-   Asume tasas de transmisión y recuperación **constantes**.

-   Limitaciones:

    -   No considera la **variabilidad individual**.

    -   No pueden aplicarse a **poblaciones abiertas** (con migraciones o nacimientos).

    -   Resultados sensibles a cambios en **parámetros estimados**.

------------------------------------------------------------------------

### Modelos **probabilísticos** (***regresión*****)**

-   Asumen que los fenómenos observados no son totalmente predecibles, y que ciertas variables están sujetas al azar (**componente aleatorio**).

-   No predicen un único resultado; en cambio, generan una **distribución de posibles resultados**.

-   Los resultados para un conjunto de datos pueden variar cada vez que se ejecute el modelo.

-   Los resultados se expresan en términos de probabilidades.

------------------------------------------------------------------------

### Componentes de un modelo de regresión

-   **Variable dependiente** ($Y$): también llamada variable respuesta o resultado, representa el **efecto observado** de un proceso o interacción causal.

-   **Variable/s independiente/s** ($X$) : también llamadas variables explicativas o predictores, representan la **influencia del proceso o interacción** que se está midiendo sobre la variable dependiente.

-   **Error aleatorio** ($\epsilon$): **variabilidad no explicada** por las variables independientes, atribuible a diferencias individuales y/o a errores de medición o calibración. Puede controlarse aumentando el tamaño de muestra.

------------------------------------------------------------------------

#### **Modelos probabilísticos paramétricos**

-   Asumen que los datos provienen de una distribución con un número fijo de parámetros (media, varianza, etc.).
-   Requieren hacer suposiciones explícitas sobre la forma de la distribución de los datos.
-   Ejemplos:
    -   Regresión lineal
    -   Análisis de la varianza (ANOVA)
    -   Regresión logística

------------------------------------------------------------------------

#### **Modelos probabilísticos no paramétricos**

-   Se utilizan para analizar datos cuya distribución subyacente es desconocida o no sigue una distribución estándar.

-   Son más flexibles pero requieren de mayor cantidad de datos.

-   Ejemplos:

    -   Curvas de Kaplan Meier

    -   Regresión de Cox (semiparamétrica)

------------------------------------------------------------------------

## Modelo lineal general

-   Modela la relación lineal entre una variable respuesta numérica con distribución normal ($Y$) y una o más variables independientes.

-   Su marco estadístico incluye:

    -   **Regresión lineal simple**: relación entre $Y$ y una variable independiente numérica.

    -   **Análisis de la varianza (ANOVA)**: diferencia en la media de $Y$ respecto a los niveles de una variable categórica.

    -   **Regresión múltiple** y **análisis de la covarianza**: relación entre $Y$ y 2 o más variables independientes.

------------------------------------------------------------------------

-   Matemáticamente se representa como:

    $$
    Y_i = X_i\beta + \epsilon_i
    $$

-   Donde:

    -   $Y_i$ : vector de valores de la variable dependiente.

    -   $X_i$: matriz de diseño que contiene las variables independientes.

    -   $\beta$: vector de coeficientes (parámetros).

    -   $\epsilon_i$: error aleatorio.

------------------------------------------------------------------------

### Supuestos del modelo lineal general

-   **Linealidad**: La relación entre la variable dependiente y cada variable independiente es lineal.

-   **Independencia**: Las observaciones deben ser independientes entre sí.

-   **Homoscedasticidad**: La varianza de los errores es constante para todos los valores de las variables independientes.

-   **Normalidad de los errores**: Los errores (residuales) del modelo deben seguir una distribución normal.

------------------------------------------------------------------------

### Regresión lineal simple

-   Expresa la relación lineal entre $Y$ y una variable independiente continua ($X$).
-   Veamos nuevamente el *scatterplot* que mostraba la covarianza entre la tasa de mortalidad por cáncer (`target_death_rate`) y la pobreza porcentual (`poverty_percent`).

------------------------------------------------------------------------

#### *Scatterplot*: tasa de mortalidad vs. pobreza porcentual

```{r}
# Carga datos
datos <- read.csv2("cancer.txt")

# Genera scatter plot
datos |> 
  ggplot(mapping = aes(x = poverty_percent, 
                       y = target_death_rate)) +
  
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  
  labs(x = "Pobreza (%)", 
       y = "Tasa de mortalidad por cáncer") +
  theme_minimal()
```

------------------------------------------------------------------------

-   La relación entre estas dos variables podría expresarse mediante la ecuación de la recta:

    $$ Y = \beta_0 + \beta_1X + \epsilon $$

-   Donde:

    -   $Y$ es la variable dependiente (`target_death_rate`).

    -   $X$ es la variable independiente (`poverty_percent`).

    -   $\beta_0$ es el intercepto.

    -   $\beta_1$ es la pendiente de la recta.

    -   $\epsilon$ es el término error aleatorio.

------------------------------------------------------------------------

#### Definamos ahora los términos de la ecuación...

-   **Intercepto** ($\beta_0$):

    -   Representa el valor esperado de $Y$ cuando $X=0$.
    -   Generalmente no tiene una interpretación realista.

-   **Pendiente** ($\beta_1$):

    -   Mide la tasa de cambio $Y$ por unidad de cambio de $X$.
    -   Si $\beta_1$ toma valores positivos, $X$ e $Y$ tienen **relación directa**, mientras que si $\beta_1$ es negativa se trata de una **relación inversa**.
    -   Sobre este término se basan las hipótesis del modelo:
        -   $H_0: \beta_1 = 0$ (No hay relación entre $X$ e $Y$)
        -   $H_1: \beta_1 \not= 0$ (Relación lineal entre $X$ e $Y$)

------------------------------------------------------------------------

-   **Error aleatorio** ($\epsilon$)

    -   Variación en la medición de un valor que se debe al azar. 

    -   El modelo lineal asume que los errores son independientes y están idénticamente distribuidos (i.i.d) y siguen una distribución normal:

        $$
        \epsilon \stackrel{i.i.d}{\sim} N(0, \sigma^2)
        $$

------------------------------------------------------------------------

### Método de los mínimos cuadrados

-   Técnica estadística para encontrar la **línea de regresión** que mejor ajusta un conjunto de datos.

-   Se basa en minimizar la suma de los cuadrados de las diferencias (**errores**) entre los valores observados y los valores predichos por el modelo.

-   La precisión del ajuste se evalúa usando el coeficiente de determinación ($R^2$).

------------------------------------------------------------------------

-   Puede visualizarse en el *scatterplot* agregando la distancia los valores observados y los predichos:

    ```{r}
    # Modifica datos
    datos1 <- sample_n(datos, 25)

    # Ajusta modelo
    fit <- lm(target_death_rate ~ poverty_percent, data = datos1)

    # Añade datos ajustados
    datos1 <- datos1 |> 
      mutate(y_fit = predict(fit))

    # Genera scatter plot con residuales
    datos1 |> 
      ggplot(mapping = aes(x = poverty_percent, 
                           y = target_death_rate)) +
      
      geom_segment(aes(xend = poverty_percent, yend = y_fit),
                   linetype = "dashed") +
      
      geom_point(color = "#105185", size = 3, alpha = .8) +
      geom_smooth(method = "lm", se = F, color = "#021326") +
      
      
      labs(x = "Pobreza porcentual", 
           y = "Tasa de mortalidad por cáncer") +
      theme_minimal()
    ```

------------------------------------------------------------------------

#### Coeficiente de determinación ($R^2$)

-   Indica la proporción de la variabilidad de $Y$ que explica el modelo de regresión lineal:

    $$
    R^2 = 1- \frac{STC}{SCE}
    $$

    -   Donde:

        -   **SCE** es la suma de errores cuadrados, es decir, la diferencia entre los valores observados y los predichos por el modelo.
        -   **STC** es la suma total de cuadrados, variabilidad total de los valores observados con respecto a su media.

------------------------------------------------------------------------

**Interpretación**

-   $R^2 = 1$: indica un ajuste perfecto, donde el modelo explica el 100% de la variabilidad de $Y$.

-   $R^2 = 0$: significa que el modelo no explica ninguna variabilidad de $Y$, y es equivalente a tomar la media de $Y$ como predicción constante.

-   **Valores intermedios**: cuanto más cerca esté $R^2$ de 1, mejor será el ajuste del modelo.

::: {.callout-important .fragment appearance="simple"}
## Aunque un valor alto de $R^2$ indica un buen ajuste, no garantiza que el modelo sea adecuado, ya que podría estar sobreajustado o faltar una relación causal.
:::

------------------------------------------------------------------------

### Residuales

-   Los residuos o residuales representan la diferencia entre los valores observados y los ajustados por el modelo.
-   Tienen las siguientes características:
    -   **Promedio cero**: La suma de los residuales de un modelo de regresión lineal debe ser igual a cero.
    -   **Normalidad**: Se espera que los residuales se distribuyan normalmente.
    -   **Independencia**: No debe haber patrones evidentes en su distribución.
    -   **Homocedasticidad**: No debe haber patrones sistemáticos en la dispersión de los residuales.

------------------------------------------------------------------------

-   Los residuales se analizan gráficamente usando las siguientes herramientas:

    -   **Gráficos Q-Q**: Comparan la distribución de los residuales con una distribución normal teórica.

    -   **Histogramas:** Ayudan a visualizar si los residuales se distribuyen normalmente.

    -   **Gráficos de dispersión**: Muestran los residuales frente a los valores predichos. Un patrón aleatorio sugiere independencia y homocedasticidad.

    -   **Gráficos de *outliers***: muestran la presencia de valores extremos (*outliers*) o influyentes que puedan distorsionar las estimaciones del modelo.

------------------------------------------------------------------------

#### Normalidad y linealidad

```{r}
# Modelo lineal
fit <- lm(target_death_rate ~ poverty_percent, data = datos)

# Gráfico normalidad y linealidad
check_model(fit, check = c("qq", "linearity"))
```

------------------------------------------------------------------------

#### Homocedasticidad y valores extremos

```{r}
# Gráfico homocedasticidad y outliers
check_model(fit, check = c("homogeneity", "outliers"))
```

## Ejemplo en R Commander

-   Activar RCommander (`library(Rcmdr`) y el plugin `KMggplot2`.

-   Cargar los datos del ejemplo de covarianza ("`cancer.txt`").

-   Vamos a **Estadísticos \> Ajuste de Modelos \> Modelo Lineal**.

-   Seleccionamos `target_death_rate` como variable dependiente y `median_age` como variable explicativa.

------------------------------------------------------------------------

-   Apretamos **Aplicar** o **Aceptar** y nos aparecerá lo siguiente:

    ![](images/clipboard-816084763.png)

------------------------------------------------------------------------

-   **Analizemos las partes de la salida del modelo...**

    ```{r}
    mod1 <- lm(target_death_rate ~ median_age, data = datos)

    summary(mod1)
    ```

------------------------------------------------------------------------

-   `Call`: es la fórmula que usamos para definir el modelo.

-   `Residuals`: resumen de la distribución de los residuales (mínimo, Q1, mediana, Q3, máximo).

-   `Coefficients`:

    -   `Estimate`: valores del intercepto y de la pendiente

    -   `Std. error`: errores estándar del intercepto y la pendiente.

    -   `t value`: estadístico $t$

    -   `Pr (>|t|)`: *p*-valor de probabilidad dada la hipótesis nula que los coeficientes sean iguales a cero.

------------------------------------------------------------------------

-   `Residual standard error`: Error estándar de los residuos con sus grados de libertad

-   `Multiple R-squared`: Coeficiente de determinación $R^2$

-   `Adjusted R-squared`: Coeficiente $R^2$ ajustado

-   `F-statistic`: estadístico $F$ sobre la hipótesis nula que el cociente entre la varianza de la ecuación de regresión y la varianza de los residuos es igual a 1.

-   `p-value`: p-valor del estadistico $F$.

------------------------------------------------------------------------

-   Si en lugar de los coeficientes y sus errores estándar quisiéramos visualizar los intervalos de confianza, debemos calcularlos desde **Modelos \> Intervalos de Confianza:**

    ![](images/clipboard-3010459011.png)

------------------------------------------------------------------------

Podemos chequear si la relación lineal es estadísticamente significativa desde **Modelos \> Test de hipótesis \> Tabla ANOVA**

```{r}
anova(mod1)
```

------------------------------------------------------------------------

-   Para el análisis de residuales vamos a **Herramientas \> Cargar paquetes** y seleccionamos la librería `performance`.

-   A continuación escribimos el siguiente comando y presionamos **Ejecutar**:

    `check_model(mod1)`

-   Se nos abrirá una nueva ventana con la salida del análisis de residuales.

------------------------------------------------------------------------

```{r}
check_model(mod1)
```

------------------------------------------------------------------------

-   Los siguientes comandos nos muestran la interpretación de los gráficos de residuos:

    -   `check_normality(mod1)`

        ```{r}
        check_normality(mod1)
        ```

    -   `check_heteroscedasticity(mod1)`

        ```{r}
        check_heteroscedasticity(mod1)
        ```

    -   `check_outliers(mod1)`

        ```{r}
        check_outliers(mod1)
        ```

------------------------------------------------------------------------

-   Interpretación de los resultados:

    -   La tasa de mortalidad por cáncer (`target_death_rate`) disminuye significativamente (p = 0,039) a medida que aumenta la mediana de edad (`median_age`).

    -   Este modelo solo explica un porcentaje muy bajo de la variabilidad observada en los datos ($R^2 =$ `r r2(mod1)[2]`).

    -   Los residuos cumplen con los supuestos de normalidad, linealidad y homogeneidad de varianza.

    -   No se detectaron valores extremos.

------------------------------------------------------------------------

### Análisis de la varianza (ANOVA)

-   Compara la media de la variable independiente $Y$ entre niveles de una variable explicativa categórica ($X$).

-   Se basa en la hipótesis estadística de que los grupos son distintos entre sí:

    -   $H_0: \mu_1 = \mu_2 = ... = \mu_i$ (la media de $Y$ no difiere entre grupos)

    -   $H_1: \mu_1 \not= \mu_2 \not= ...\not=\mu_i$ (la media de $Y$ difiere entre grupos)

------------------------------------------------------------------------

-   Divide la variación total en:

    -   variación entre muestras

    -   variación dentro de las muestras

------------------------------------------------------------------------

### Ejemplo en R Commander

-   Vamos a **Estadísticos \> Ajuste de Modelos \> Modelo Lineal**.

-   Seleccionamos `target_death_rate` como variable dependiente y `median_age_cat` como variable explicativa y apretamos **Aceptar** o **Aplicar.**

-   A continuación vamos a **Modelos \> Test de hipótesis \> Tabla ANOVA.**

    ```{r}
    mod2 <- lm(target_death_rate ~ median_age_cat, data = datos)

    anova(mod2)
    ```

------------------------------------------------------------------------

-   Sin embargo, en el ANOVA nos interesa saber **cuáles grupos** son diferentes entre sí mediante tests de comparaciones múltiples.

-   Para que R Commander nos permita realizar estos test, debemos ingresar el modelo desde **Estadísticos \> Medias \> ANOVA de un factor** y marcar la opción "Comparaciones dos a dos de las medias".

------------------------------------------------------------------------

-   Nos aparecerá en la consola una salida equivalente a la obtenida ajustando como modelo lineal más el resultado de los test de comparaciones múltiples:

    ![](images/clipboard-4111011610.png)

------------------------------------------------------------------------

```{r}
library(multcomp)
glht(mod2, linfct = mcp(median_age_cat = "Tukey"))
```
